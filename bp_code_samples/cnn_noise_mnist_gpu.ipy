# coding: utf-8
import torch
import torchvision
import torchvision.transforms as transforms

from torchdp import PrivacyEngine

num_epochs=10
learning_rate=0.001
clipRate = 4.0
epsilon = 0.5
bs = 4

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5), (0.5))])
	 
trainset = torchvision.datasets.MNIST(root='./dataMnist', train=True,
                                        download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=0)

testset = torchvision.datasets.MNIST(root='./dataMnist', train=False,
                                       download=True, transform=transform)

testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=False, num_workers=0)

#classes = ('plane', 'car', 'bird', 'cat',
#           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

classes = ('0','1','2','3','4','5','6','7','8','9')

import matplotlib.pyplot as plt
import numpy as np

# functions to show an image


#def imshow(img):
#    img = img / 2 + 0.5     # unnormalize
#    npimg = img.numpy()
#    plt.imshow(np.transpose(npimg, (1, 2, 0)))
#    plt.show()


# get some random training images
#dataiter = iter(trainloader)
#images, labels = dataiter.next()

# show images
#imshow(torchvision.utils.make_grid(images))
# print labels
#print(' '.join('%5s' % classes[labels[j]] for j in range(4)))

import torch.nn as nn
import torch.nn.functional as F

class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.drop_out = nn.Dropout()
        self.fc1 = nn.Linear(7 * 7 * 64, 1000)
        self.fc2 = nn.Linear(1000, 10)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.drop_out(out)
        out = self.fc1(out)
        out = self.fc2(out)
        return out  


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")		


net = ConvNet()
import torch.optim as optim

#criterion = nn.CrossEntropyLoss()
# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
#optimizer = optim.Adam(net.parameters(), lr=learning_rate)

#import torch.optim as optim

criterion = nn.CrossEntropyLoss()
#criterion = nn.CrossEntropyLoss(reduction='none')
# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(net.parameters(), lr=learning_rate)

#net = Net()
        


# Assuming that we are on a CUDA machine, this should print a CUDA device:

print(device)

net.to(device)

privacy_engine = PrivacyEngine(
    net,
    bs,
    sample_size=len(trainloader.dataset),
    alphas=[1, 10, 100],
    noise_multiplier=epsilon,
    max_grad_norm=clipRate,
).to(device)

privacy_engine.attach(optimizer)

for epoch in range(num_epochs):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), data[1].to(device)
        #inputs2 = inputs.view(-1,28*28)
        #inputs2 = inputs.view(-1,28*28)
#         print(inputs.size())

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
		#outputs = []
		#for i in range(inputs.size()[0]):
		#	outputs.append(net(inputs[i]))
        outputs = net(inputs)
        #if i % 2000 == 1999:    # print every 2000 mini-batches
        #    print('outputs: ',outputs)
		
		#should get loss for each input
        #losses = []
        #print(outputs.size()[0])
        #print(labels.size())
        #print(labels)
        #print(outputs[0])
        #for i in range(outputs.size()[0]):
        #    losses.append(criterion(outputs[i],labels[i]))
        loss = criterion(outputs, labels)
        #if i % 2000 == 1999:    # print every 2000 mini-batches
        #    print('pre-clipping grad?: ',loss)
			
        #print(type(loss))
		
		#try grad clipping the loss for each input
        #for i in loss:
        #nn.utils.clip_grad_norm(loss,clipRate)
		
		#try noise injection
        #meanClippedGrad = torch.mean(loss,0,True)
        #noise = torch.normal(0,epsilon,size=meanClippedGrad.size())
		
        #finalGrad = meanClippedGrad + noise
		
        #if i % 2000 == 1999:    # print every 2000 mini-batches
        #    print('post-clipping grad?: ', finalGrad)
		
        loss.backward()
        #finalGrad.backward()
		
		#trying grad clipping
        #nn.utils.clip_grad_norm(net.parameters(), clipRate)
		
		#try noise injection
		#noise = torch.normal(torch.tensor([0.0]),torch.tensor([epsilon]))
		#noise = torch.normal(0,epsilon,size=)
		
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')

#PATH = './gpu_cifar_net.pth'
#torch.save(net.state_dict(), PATH)

#dataiter = iter(testloader)
#images, labels = dataiter.next()

# print images
#imshow(torchvision.utils.make_grid(images))
#print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))

#net = ConvNet()
#net.load_state_dict(torch.load(PATH))

#outputs = net(images.view(-1,3*32*32))

#_, predicted = torch.max(outputs, 1)

#print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]
#                              for j in range(4)))

#net.to(device)
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
#        images, labels = data
        images, labels = data[0].to(device), data[1].to(device)
        #images2 = images.view(-1,28*28)
        #images2 = images.view(-1,28*28)
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))

#print(device)

class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        #images, labels = data
        images, labels = data[0].to(device), data[1].to(device)
        #outputs = net(images.view(-1,28*28))
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze()
        for i in range(4):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] += 1


for i in range(10):
    print('Accuracy of %5s : %2d %%' % (
        classes[i], 100 * class_correct[i] / class_total[i]))
